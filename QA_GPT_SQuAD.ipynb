{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0ecdbaf-27d0-48af-b78f-92e62fa7a43e",
   "metadata": {},
   "source": [
    "# Question Answering with SQuAD Dataset and Pre-Trained GPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f4de1d-a962-4907-ae78-f8db192686be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "#plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.figsize'] = (4, 2)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "#from models.transformer import GPT\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "    \n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import re\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'USING DEVICE: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274c9bfd-09a5-478b-83f5-24d9dc659a4f",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46a81acb-e831-4e1c-be14-3a6768960043",
   "metadata": {},
   "outputs": [],
   "source": [
    "steplr = {\n",
    "    'type': 'StepLR',\n",
    "    #'step_size': 2,\n",
    "    'step_size': 800,\n",
    "    #'gamma': 0.80,\n",
    "    'gamma': 0.99,\n",
    "}\n",
    "\n",
    "reduce_lr_on_plateau = {\n",
    "    'type': 'ReduceLROnPlateau',\n",
    "    'mode': 'min',\n",
    "    'factor': 0.1,\n",
    "    'patience': 3,\n",
    "    'cooldown': 0,\n",
    "    'min_lr': 1e-7,\n",
    "}\n",
    "\n",
    "# TODO: revise the max_seq_len and context_size\n",
    "hyperparameters = {\n",
    "    'seed': 99999,\n",
    "    'batch_size': 32,\n",
    "    #'vocab_size': 50_257,\n",
    "    #'max_seq_len': 256, \n",
    "    'context_size': 256,\n",
    "    'split_ratio': 0.75,\n",
    "    'num_epochs': 2,\n",
    "    #'num_training_iters': 5_000,\n",
    "    #'num_validation_iters': 1_000,\n",
    "    'optimizer': {\n",
    "        'learning_rate': 1e-4, # lower than default for fine tuning\n",
    "        'momentum': 0.9, # SGD\n",
    "        'optimizer_betas': (0.9, 0.999), # Adam, AdamW\n",
    "        'weight_decay': 1e-2, # AdamW\n",
    "    },\n",
    "    'clip_grad_norm': 1.0,\n",
    "    'grad_accum_iter': 4,\n",
    "    'learning_rate_sched_config': reduce_lr_on_plateau,\n",
    "    #'dataset_dir': '../data/trwiki-20231120-pages-articles/',\n",
    "    'model_base_name': 'QA_GPT2_SQuAD_FineTune',\n",
    "    'model_config': {}, # removed from above!\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d1ddafe-7a18-4864-820f-88c2c9892c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False # !!!!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "seed_everything(hyperparameters['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fabb45d-cd29-460f-bd6a-39bb05db2488",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bba9e30-bbc4-4077-8e28-7d5324784d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('openai-community/gpt2')\n",
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3105205d-785b-4824-9859-5bbb3d751d44",
   "metadata": {},
   "source": [
    "## Set Special Tokens for QA "
   ]
  },
  {
   "cell_type": "raw",
   "id": "3766b8bd-7932-459e-8edf-1e0327163e1b",
   "metadata": {},
   "source": [
    "#special_tokens_dict = {'additional_special_tokens': ['[CONTEXT]', '[QUESTION]', '[ANSWER]', '[END]']}\n",
    "# [CONTEXT] --> [CLS]\n",
    "# [END] --> [SEP]\n",
    "special_tokens_dict = {'additional_special_tokens': ['[QUESTION]', '[ANSWER]']}\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f9e7273-4f5c-4055-a701-a68a7de4dbc5",
   "metadata": {},
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b70e7cfa-1375-4980-8f86-d61eaa7a394b",
   "metadata": {},
   "source": [
    "#tokenizer.save_pretrained('QA_tokenizer')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34361283-891b-4f35-bad1-f06a97f5c9d4",
   "metadata": {},
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64d16951-0833-4eed-8ca4-16a2a4938b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token # !!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a75d22f-5a06-4c60-840d-9ef1e01513fb",
   "metadata": {},
   "source": [
    "### Update hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a061458-cf7d-4f94-866d-a4bd21b91f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters['model_config']['vocab_size'] = len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fda28b5-d5d9-49d7-81ec-5223295d2972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790d7fd8-1ed7-4100-b02c-50f828be5e37",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4331e12c-8aa0-4464-9fd9-5cbc6fdb1f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87599\n",
      "10570\n"
     ]
    }
   ],
   "source": [
    "squad_full = load_dataset('rajpurkar/squad')\n",
    "\n",
    "squad_train = squad_full['train']\n",
    "squad_val = squad_full['validation']\n",
    "\n",
    "print(len(squad_train))\n",
    "print(len(squad_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfd8f11d-9f40-42a0-8658-27ef3e39931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_df = squad_train.to_pandas()\n",
    "dataset_val_df = squad_val.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b495178-f0e3-429e-9b59-c58674323bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>{'text': ['Saint Bernadette Soubirous'], 'answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>{'text': ['a copper statue of Christ'], 'answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>{'text': ['the Main Building'], 'answer_start'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>{'text': ['a Marian place of prayer and reflec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>{'text': ['a golden statue of the Virgin Mary'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id                     title  \\\n",
       "0  5733be284776f41900661182  University_of_Notre_Dame   \n",
       "1  5733be284776f4190066117f  University_of_Notre_Dame   \n",
       "2  5733be284776f41900661180  University_of_Notre_Dame   \n",
       "3  5733be284776f41900661181  University_of_Notre_Dame   \n",
       "4  5733be284776f4190066117e  University_of_Notre_Dame   \n",
       "\n",
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                            question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                  What is the Grotto at Notre Dame?   \n",
       "4  What sits on top of the Main Building at Notre...   \n",
       "\n",
       "                                             answers  \n",
       "0  {'text': ['Saint Bernadette Soubirous'], 'answ...  \n",
       "1  {'text': ['a copper statue of Christ'], 'answe...  \n",
       "2  {'text': ['the Main Building'], 'answer_start'...  \n",
       "3  {'text': ['a Marian place of prayer and reflec...  \n",
       "4  {'text': ['a golden statue of the Virgin Mary'...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37a1ef6b-4618-448d-9e6c-0ca74abed9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be4db0acb8001400a502ec</td>\n",
       "      <td>Super_Bowl_50</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
       "      <td>{'text': ['Denver Broncos', 'Denver Broncos', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be4db0acb8001400a502ed</td>\n",
       "      <td>Super_Bowl_50</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Which NFL team represented the NFC at Super Bo...</td>\n",
       "      <td>{'text': ['Carolina Panthers', 'Carolina Panth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be4db0acb8001400a502ee</td>\n",
       "      <td>Super_Bowl_50</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Where did Super Bowl 50 take place?</td>\n",
       "      <td>{'text': ['Santa Clara, California', 'Levi's S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56be4db0acb8001400a502ef</td>\n",
       "      <td>Super_Bowl_50</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Which NFL team won Super Bowl 50?</td>\n",
       "      <td>{'text': ['Denver Broncos', 'Denver Broncos', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56be4db0acb8001400a502f0</td>\n",
       "      <td>Super_Bowl_50</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>What color was used to emphasize the 50th anni...</td>\n",
       "      <td>{'text': ['gold', 'gold', 'gold'], 'answer_sta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id          title  \\\n",
       "0  56be4db0acb8001400a502ec  Super_Bowl_50   \n",
       "1  56be4db0acb8001400a502ed  Super_Bowl_50   \n",
       "2  56be4db0acb8001400a502ee  Super_Bowl_50   \n",
       "3  56be4db0acb8001400a502ef  Super_Bowl_50   \n",
       "4  56be4db0acb8001400a502f0  Super_Bowl_50   \n",
       "\n",
       "                                             context  \\\n",
       "0  Super Bowl 50 was an American football game to...   \n",
       "1  Super Bowl 50 was an American football game to...   \n",
       "2  Super Bowl 50 was an American football game to...   \n",
       "3  Super Bowl 50 was an American football game to...   \n",
       "4  Super Bowl 50 was an American football game to...   \n",
       "\n",
       "                                            question  \\\n",
       "0  Which NFL team represented the AFC at Super Bo...   \n",
       "1  Which NFL team represented the NFC at Super Bo...   \n",
       "2                Where did Super Bowl 50 take place?   \n",
       "3                  Which NFL team won Super Bowl 50?   \n",
       "4  What color was used to emphasize the 50th anni...   \n",
       "\n",
       "                                             answers  \n",
       "0  {'text': ['Denver Broncos', 'Denver Broncos', ...  \n",
       "1  {'text': ['Carolina Panthers', 'Carolina Panth...  \n",
       "2  {'text': ['Santa Clara, California', 'Levi's S...  \n",
       "3  {'text': ['Denver Broncos', 'Denver Broncos', ...  \n",
       "4  {'text': ['gold', 'gold', 'gold'], 'answer_sta...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d1ecd33-7e68-47bd-b1de-c0fed4346996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample_qa(df, sample_id):\n",
    "    print('CONTEXT:')\n",
    "    print(df['context'][sample_id])\n",
    "    print('QUESTION:')\n",
    "    print(df['question'][sample_id])\n",
    "    print('ANSWER:')\n",
    "    print(df['answers'][sample_id]['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21a7cc72-92cd-446c-b0ef-e8cba105f5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT:\n",
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "QUESTION:\n",
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "ANSWER:\n",
      "Saint Bernadette Soubirous\n"
     ]
    }
   ],
   "source": [
    "print_sample_qa(dataset_train_df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baeb1343-9fcf-4249-8bcd-132004b06555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT:\n",
      "After Hurricane Katrina in 2005, Beyoncé and Rowland founded the Survivor Foundation to provide transitional housing for victims in the Houston area, to which Beyoncé contributed an initial $250,000. The foundation has since expanded to work with other charities in the city, and also provided relief following Hurricane Ike three years later.\n",
      "QUESTION:\n",
      "How much did Beyonce initially contribute to the foundation?\n",
      "ANSWER:\n",
      "$250,000\n"
     ]
    }
   ],
   "source": [
    "print_sample_qa(dataset_train_df, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958e83de-505d-444d-aa48-aa11e8639efb",
   "metadata": {},
   "source": [
    "# Prompt Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbc068e0-63fa-4211-8b98-50581fe59fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# closed (context dependent) question answering\n",
    "#make_prompt_df = lambda df_row:f'{df_row[\"context\"]} [QUESTION] {df_row[\"question\"]} [ANSWER] {df_row[\"answers\"][\"text\"][0]}'\n",
    "\n",
    "# open-ended (next token prediction) question answering\n",
    "make_prompt_df = lambda df_row:f'[QUESTION] {df_row[\"question\"]} [ANSWER] {df_row[\"answers\"][\"text\"][0]} [END]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c380a06-16c9-428e-b52a-3de74dc843f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use context as language modelling\n",
    "#dataset_train_df['prompt'] = dataset_train_df[['context']]\n",
    "#dataset_val_df['prompt'] = dataset_train_df[['context']]\n",
    "\n",
    "# closed (context dependent) question answering\n",
    "#dataset_train_df['prompt'] = dataset_train_df[['context', 'question', 'answers']].apply(make_prompt_df, axis=1)\n",
    "#dataset_val_df['prompt'] = dataset_val_df[['context', 'question', 'answers']].apply(make_prompt_df, axis=1)\n",
    "\n",
    "# open-ended (next token prediction) question answering\n",
    "dataset_train_df['prompt'] = dataset_train_df[['question', 'answers']].apply(make_prompt_df, axis=1)\n",
    "dataset_val_df['prompt'] = dataset_val_df[['question', 'answers']].apply(make_prompt_df, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01e4a12-9108-4075-a38a-ef854f716bd7",
   "metadata": {},
   "source": [
    "# Set Maximum Sequence (Token) Length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ca7a696-e1ef-4cee-8879-e32a0181247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_token_len(df_train, df_val, tokenizer):\n",
    "    # keep only words and punctuation\n",
    "    pattern = r'\\w+|[^\\w\\s]'\n",
    "    \n",
    "    def text_process(text):\n",
    "        result = ' '.join(re.findall(pattern, text))\n",
    "        return len(tokenizer.encode(result))\n",
    "    \n",
    "    train_token_len = df_train['prompt'].map(lambda x:text_process(x)).values\n",
    "    val_token_len = df_val['prompt'].map(lambda x:text_process(x)).values\n",
    "\n",
    "    total_token_len = np.concat((train_token_len, val_token_len))\n",
    "    \n",
    "    print(f'Mean: {total_token_len.mean()}')\n",
    "    print(f'Std: {total_token_len.std()}')\n",
    "    print(f'Max: {total_token_len.max()}')\n",
    "    print(f'Mean + 2*Std {total_token_len.mean() + 2*total_token_len.std()}')\n",
    "    print(f'Mean + 3*Std {total_token_len.mean() + 3*total_token_len.std()}')\n",
    "    \n",
    "    return int(total_token_len.max()), round(total_token_len.mean() + 3*total_token_len.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6f551f8-2df5-42c3-b62a-9a86efb21e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 28.756613594923042\n",
      "Std: 5.7988131571338775\n",
      "Max: 99\n",
      "Mean + 2*Std 40.354239909190795\n",
      "Mean + 3*Std 46.153053066324674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(99, 46)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_len_max, token_len_3std = find_optimal_token_len(dataset_train_df, dataset_val_df, tokenizer)\n",
    "token_len_max, token_len_3std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d2246a5-c6a7-4318-ad33-eabe6b63f73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 46)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_len_max, token_len_3std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576ecfb1-e202-4375-828e-a7c210521359",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "007de678-942a-4a5c-b4d7-90f5cd6ba637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQuADDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_seq_len):\n",
    "        \n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        selected_row = self.df.iloc[idx]\n",
    "        prompt_text = selected_row['prompt']\n",
    "        \n",
    "        tokenized_dict = self.tokenizer.encode_plus(\n",
    "            prompt_text,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_seq_len,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "\n",
    "        input_ids = torch.tensor(tokenized_dict['input_ids'], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(tokenized_dict['attention_mask'], dtype=torch.float32)\n",
    "\n",
    "        #### SHIFT BY ONE TOKEN ####\n",
    "        input_input_ids = input_ids[:-1]\n",
    "        input_attention_mask = attention_mask[:-1]\n",
    "        target_input_ids = input_ids[1:]\n",
    "        ############################\n",
    "                \n",
    "        return input_input_ids, input_attention_mask, target_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f41203c5-dfe7-4561-a747-82ad77a43984",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = SQuADDataset(\n",
    "    df=dataset_train_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=token_len_3std,\n",
    ")\n",
    "\n",
    "dataset_val = SQuADDataset(\n",
    "    df=dataset_val_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=token_len_3std,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "171f3845-7f14-45c8-8622-89666270b4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([45]), torch.Size([45]), torch.Size([45]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i, a, o = dataset_train[0]\n",
    "i.shape, a.shape, o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bbb3a0d-df00-4520-a37d-21e14852361f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[QUESTION] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [ANSWER] Saint Bernadette Soubirous [END]<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(i.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "206367bc-c0ff-467f-a421-fc5a576cc8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QUESTION] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [ANSWER] Saint Bernadette Soubirous [END]<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(o.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db7d483-96e4-4410-855b-e7293c5cad20",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e25eac00-04b1-4baf-86d2-b417dbc231a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(\n",
    "    dataset_train, \n",
    "    batch_size=hyperparameters['batch_size'], \n",
    "    shuffle=True,\n",
    "    #num_workers=1, \n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    dataset_val, \n",
    "    batch_size=hyperparameters['batch_size'], \n",
    "    shuffle=True,\n",
    "    #num_workers=1, \n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f01a4a3-1ba9-47eb-8071-c8d4015833d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 45]), torch.Size([32, 45]), torch.Size([32, 45]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ib, ab, ob = next(iter(dataloader_train))\n",
    "ib.shape, ab.shape, ob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d55d15f-b65d-48fb-8c16-b24d8c885510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 45]), torch.Size([32, 45]), torch.Size([32, 45]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ib, ab, ob = next(iter(dataloader_val))\n",
    "ib.shape, ab.shape, ob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9917b-0e10-4805-bed2-3c3cab2da16a",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "994d6136-601e-44ed-ad1d-c63cc33998ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "#model = GPT(**hyperparameters['model_config'])\n",
    "model = GPT2LMHeadModel.from_pretrained('openai-community/gpt2')\n",
    "\n",
    "print(f'Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d7e395f-491b-48da-b3e7-eb337dc69c7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer parameters: 124,439,808\n",
      "lm_head parameters: 38,597,376\n"
     ]
    }
   ],
   "source": [
    "for n, m in model.named_children():\n",
    "    print(f'{n} parameters: {sum(p.numel() for p in m.parameters() if p.requires_grad):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4740fd36-e502-4bd9-92f4-0897567addde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 50257])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_batch_size, _seq_len = 1, 20\n",
    "\n",
    "pred = model(torch.randint(low=0, high=hyperparameters['model_config']['vocab_size'], size=(_batch_size, _seq_len))).logits\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab47bf8e-452d-49dd-9870-04c4ea377c3c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e838109-19e6-4fd4-b3fa-dd99b4084625",
   "metadata": {},
   "outputs": [],
   "source": [
    "PBAR_UPDATE_FREQ = 60\n",
    "\n",
    "list_avg = lambda l: sum(l)/len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d2d196f-7ed2-4f5e-99bb-e860cebc193d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_iter_amp(dataloader, model, optimizer, criterion, scaler, epoch, clip_grad_norm, pbar_update_freq, grad_accum_iter, device):\n",
    "    model.train()\n",
    "    \n",
    "    avg_loss = []\n",
    "    count = 0\n",
    "\n",
    "    pbar = tqdm(dataloader, unit=' batch', position=0, leave=True)\n",
    "    \n",
    "    pbar.set_description(f'Epoch: {epoch}, Train')\n",
    "        \n",
    "    for batch_idx, (input_token_ids_batch, attention_mask_batch, target_token_ids_batch) in enumerate(dataloader):\n",
    "\n",
    "        input_token_ids_batch = input_token_ids_batch.to(device)\n",
    "        attention_mask_batch = attention_mask_batch.to(device)\n",
    "        target_token_ids_batch = target_token_ids_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16, enabled=True):\n",
    "            pred_token_ids_batch = model(\n",
    "                input_ids=input_token_ids_batch,\n",
    "                attention_mask=attention_mask_batch,\n",
    "                #x_input=input_token_ids_batch, \n",
    "                #pad_mask=attention_mask_batch\n",
    "            ).logits ## RETURN LOGITS!\n",
    "    \n",
    "            # Combine batch and seq_len dims together to form a \"longer batch\"\n",
    "            loss = criterion(\n",
    "                pred_token_ids_batch.view(-1, pred_token_ids_batch.size(-1)),\n",
    "                target_token_ids_batch.view(-1)\n",
    "            )\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        ### CLIPPING ######\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
    "        ###################\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        avg_loss.append(loss.item())\n",
    "        count += 1\n",
    "\n",
    "        if count % pbar_update_freq  == 0:\n",
    "            #iter_accuracy = 100.0 * acc_correct / acc_total\n",
    "            #pbar.set_postfix_str(f'Loss: {list_avg(avg_loss):.4f} Acc: {iter_accuracy:.2f}')\n",
    "            pbar.set_postfix_str(f'Loss: {list_avg(avg_loss):.4f}')\n",
    "            pbar.update(pbar_update_freq)\n",
    "        \n",
    "    pbar.close()\n",
    "    \n",
    "    return list_avg(avg_loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_iter_amp(dataloader, model, criterion, epoch, pbar_update_freq, device):\n",
    "    model.eval()\n",
    "\n",
    "    avg_loss = []\n",
    "    count = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, unit=' batch', position=0, leave=True)\n",
    "    \n",
    "    pbar.set_description(f'Epoch: {epoch}, Eval')\n",
    "    \n",
    "    for input_token_ids_batch, attention_mask_batch, target_token_ids_batch in dataloader:\n",
    "\n",
    "        input_token_ids_batch = input_token_ids_batch.to(device)\n",
    "        attention_mask_batch = attention_mask_batch.to(device)\n",
    "        target_token_ids_batch = target_token_ids_batch.to(device)\n",
    "\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16, enabled=True):\n",
    "            pred_token_ids_batch = model(\n",
    "                input_ids=input_token_ids_batch,\n",
    "                attention_mask=attention_mask_batch,\n",
    "                #x_input=input_token_ids_batch,\n",
    "                #pad_mask=attention_mask_batch\n",
    "            ).logits ## RETURN LOGITS!\n",
    "        \n",
    "            # Combine batch and seq_len dims together to form a \"longer batch\"\n",
    "            loss = criterion(\n",
    "                pred_token_ids_batch.view(-1, pred_token_ids_batch.size(-1)),\n",
    "                target_token_ids_batch.view(-1)\n",
    "            )\n",
    "\n",
    "            \n",
    "        avg_loss.append(loss.item())\n",
    "        count += 1\n",
    "        \n",
    "        if count % pbar_update_freq  == 0:\n",
    "            #iter_accuracy = 100.0 * acc_correct / acc_total\n",
    "            #pbar.set_postfix_str(f'Loss: {list_avg(avg_loss):.4f} Acc: {iter_accuracy:.2f}')\n",
    "            pbar.set_postfix_str(f'Loss: {list_avg(avg_loss):.4f}')\n",
    "            pbar.update(pbar_update_freq)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    return list_avg(avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4ed104-c6b3-4fb3-80d8-6da4677a6c15",
   "metadata": {},
   "source": [
    "# Optimizer, Scheduler & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f49c04f7-72ba-46c3-b29f-3ade3578df81",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=hyperparameters['optimizer']['learning_rate'],\n",
    "    betas=hyperparameters['optimizer']['optimizer_betas'],\n",
    "    weight_decay=hyperparameters['optimizer']['weight_decay']\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), \n",
    "    lr=hyperparameters['optimizer']['learning_rate'],\n",
    "    momentum=hyperparameters['optimizer']['momentum']\n",
    "    #weight_decay=hyperparameters['optimizer']['weight_decay']\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# PAD ID IGNORE!!\n",
    "#criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.vocab['[PAD]'])\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.encode(tokenizer.pad_token)[0])\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer,\n",
    "    mode=hyperparameters['learning_rate_sched_config']['mode'],\n",
    "    factor=hyperparameters['learning_rate_sched_config']['factor'],\n",
    "    patience=hyperparameters['learning_rate_sched_config']['patience'],\n",
    "    cooldown=hyperparameters['learning_rate_sched_config']['cooldown'],\n",
    "    min_lr=hyperparameters['learning_rate_sched_config']['min_lr'],\n",
    "    #verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020529f-1f2e-4f9c-90a7-5d4721453ae7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Grad Scaler (FP16) For Automatic Mixed Precision (AMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b98d04e-20b3-4be5-ad05-a33fb9f0b9bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = torch.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d8ec21-92d5-4ac1-9799-70ee5a09fe99",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Save/Load Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4772f8c6-fca3-4836-bcbc-48972c0da3c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, root_folder, file_name, hyperparameter_dict, verbose=False):\n",
    "    os.makedirs(root_folder, exist_ok=True)\n",
    "    model_full_path = os.path.join(root_folder, file_name+'.pt')\n",
    "    \n",
    "    torch.save({\n",
    "        'hyperparameters': hyperparameter_dict,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'saved_time_unix': time.time(),\n",
    "        'saved_time_asctime': time.asctime(),\n",
    "    }, model_full_path)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Model: {file_name} is saved successfully')\n",
    "    \n",
    "    \n",
    "def load_model(model, optimizer, root_folder, file_name):\n",
    "    model_full_path = os.path.join(root_folder, file_name+'.pt')\n",
    "    #checkpoint = torch.load(model_full_path, map_location='cpu')\n",
    "    checkpoint = torch.load(model_full_path)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=True)\n",
    "    \n",
    "    #if optimizer is not None:\n",
    "    #    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    print(f'Model: {file_name} is loaded successfully')\n",
    "    \n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f68052-7f29-4d74-8c6d-902a50844ccf",
   "metadata": {},
   "source": [
    "# Language Modelling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b349e229-e3ff-4551-b09f-ae174026ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multinomial_sampling_amp(model, idx, max_new_tokens, max_seq_len, temp=1.0, topk=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -max_seq_len:]\n",
    "\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16, enabled=True):\n",
    "            \n",
    "            logits = model(input_ids=idx_cond, attention_mask=None).logits ## RETURN LOGITS!\n",
    "    \n",
    "            logits = logits[:, -1, :]\n",
    "                       \n",
    "            if topk is not None:\n",
    "                _values, _indices = F.softmax(logits/temp, dim=-1).topk(topk, dim=1)\n",
    "                #probs = F.softmax(logits/temp, dim=-1).topk(topk, dim=-1).values\n",
    "                probs = _values\n",
    "            else:\n",
    "                probs = F.softmax(logits/temp, dim=-1)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            _idx_next = torch.multinomial(probs, num_samples=1) \n",
    "    \n",
    "            idx_next = _indices[:, _idx_next[0]]\n",
    "    \n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def generate_text(model, device, n_tokens, temp, context=None, topk=None, remove_newlines=True, skip_after_end_token=True):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    if context is None:\n",
    "        context = torch.tensor([tokenizer.encode('', add_special_tokens=False)], dtype=torch.long, device=device)\n",
    "    else:\n",
    "        context = torch.tensor([tokenizer.encode(context, add_special_tokens=False)], dtype=torch.long, device=device)\n",
    "\n",
    "    _genereted = generate_multinomial_sampling_amp(\n",
    "            model,\n",
    "            context, \n",
    "            max_new_tokens=n_tokens, \n",
    "            max_seq_len=hyperparameters['context_size'], \n",
    "            temp=temp,\n",
    "            topk=topk\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(\n",
    "        _genereted[0].tolist()\n",
    "    )\n",
    "    \n",
    "    if remove_newlines:\n",
    "        generated = generated.replace('\\n', '')\n",
    "\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # END TOKEN IS ASSUMED TO BE [END]\n",
    "    if skip_after_end_token:\n",
    "        generated = generated.split('[END]')[0]\n",
    "        \n",
    "    print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2009bc8b-ddaf-46cc-b506-7f333f8ba444",
   "metadata": {},
   "source": [
    "### Start Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad36091c-b264-4279-a205-921ee4b4a88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_EPOCH = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb00850f-4f30-4cfc-98bd-18b109f928d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Pre-Trained Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d88d844-306b-4ab3-9bd1-6c0f9e2101f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint = load_model(model, optimizer, './saved_models', f\"{hyperparameters['model_base_name']}_best\")\n",
    "#START_EPOCH = checkpoint['last_epoch'] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7338dd19-989c-4f10-ac3e-249fb74b155a",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd9daadb-340a-44e4-90d8-ca1510bdc16e",
   "metadata": {
    "tags": [
     "train_output"
    ]
   },
   "outputs": [],
   "source": [
    "def start_training(start_epoch=1):\n",
    "    print(f'Start trainin from epoch: {start_epoch}')\n",
    "    \n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    \n",
    "    for epoch in range(start_epoch, hyperparameters['num_epochs']+1):\n",
    "        \n",
    "        train_loss = train_iter_amp(\n",
    "            dataloader_train, \n",
    "            model, \n",
    "            optimizer, \n",
    "            criterion, \n",
    "            scaler, \n",
    "            epoch,\n",
    "            hyperparameters['clip_grad_norm'], \n",
    "            PBAR_UPDATE_FREQ, \n",
    "            hyperparameters['grad_accum_iter'],\n",
    "            device\n",
    "        )\n",
    "        \n",
    "        val_loss = eval_iter_amp(\n",
    "            dataloader_val, \n",
    "            model, \n",
    "            criterion, \n",
    "            epoch, \n",
    "            PBAR_UPDATE_FREQ, \n",
    "            device\n",
    "        )\n",
    "        \n",
    "        print(f'Epoch: {epoch}, [LOSS] train: {train_loss:.4f}, val: {val_loss:.4f}')\n",
    "        \n",
    "        generate_text(\n",
    "            model=model, \n",
    "            device=device,\n",
    "            n_tokens=100, \n",
    "            temp=0.75, \n",
    "            context='[QUESTION] To whom did the ',\n",
    "            topk=50,\n",
    "            remove_newlines=False\n",
    "        )\n",
    "        \n",
    "        # LR Scheduling\n",
    "        lr_scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10151cb8-1e1b-46b4-ae7b-6c5a95175013",
   "metadata": {},
   "source": [
    "### Test (Before Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c1ec243-8ed8-499e-9117-c8d94dbaa7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QUESTION] How much did Beyonce initially contribute to the foundation? [ANSWER] I think she may have contributed to a lot of things. She was a very prolific artist back then, and I think that was a really large part of her influence. I remember that she gave me a piece of paper to remember the foundation, and they helped her with it. I remember she did a lot of writing to help me remember. I think that was really a part of her personality as well.\n",
      "\n",
      "AMY GOODMAN: And how did she get the money for the foundation?\n",
      "\n",
      "JACOB JONES: She used to help me fund everything. She used to be one of the most generous people I have ever met, and she doesn't know what she's going through. She didn't have a lot of money. She used to be a very creative person. She knew how to do things. I think her foundation helped her a lot.\n",
      "\n",
      "And she wasn't the only person who was involved in this. I think she had a very long list that she knew how to do. She had a list of things to do. I think she had a lot of things that she wanted to do to make this happen. And she was able to get the funding from the foundation.\n",
      "\n",
      "AMY GOODMAN: And how\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "generate_text(\n",
    "    model=model, \n",
    "    device=device,\n",
    "    n_tokens=250, \n",
    "    temp=0.75, \n",
    "    context='[QUESTION] How much did Beyonce initially contribute to the foundation? [ANSWER]',\n",
    "    topk=150,\n",
    "    remove_newlines=False,\n",
    "    skip_after_end_token=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e019d424-421f-45e9-abe8-2639d36fb651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QUESTION] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [ANSWER]\n",
      "\n",
      "QUESTION: Your Honor, I can't answer that question. I have been in France for some years, and they have all the customs, but I am not sure that I read the records.\n",
      "\n",
      "QUESTION: And do you recall at some point, at some point during the campaign, the day you said to the ambassador asked you, \"From what records would they be able to find this Virgin Mary?\"\n",
      "\n",
      "QUESTION: Of course I was curious what you said. And you know, I was not one of the people who signed the petition. I was not one of those people who was in the press or in the press in those days. And you can tell how much I was shocked. I was very surprised when I got that letter.\n",
      "\n",
      "QUESTION: I understand.\n",
      "\n",
      "QUESTION: And you know, I also wrote a letter to the ambassador and to the ambassador, to the minister of the interior. I didn't get to that point.\n",
      "\n",
      "QUESTION: You know, I was in the French press and I was a part of it. But at some point, I think, somehow you read into that, you know, a certain amount of the letter that I wrote is\n"
     ]
    }
   ],
   "source": [
    "generate_text(\n",
    "    model=model, \n",
    "    device=device,\n",
    "    n_tokens=250, \n",
    "    temp=0.75, \n",
    "    context='[QUESTION] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [ANSWER]',\n",
    "    topk=150,\n",
    "    remove_newlines=False,\n",
    "    skip_after_end_token=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c75b265-1a8e-46f7-bab2-bc920c746977",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40b29356-6a7d-40e5-b189-a695ff15b315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer.param_groups[0]['lr'] = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fedf47da-2cf5-43eb-aca6-3d10a8fb65b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start trainin from epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train:  99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 2700/2738 [07:29<00:06,  6.00 batch/s, Loss: 2.5026]\n",
      "Epoch: 1, Eval:  91%|███████████████████████████████████████████████████████████████████████████████████████████████████████▎          | 300/331 [00:15<00:01, 19.63 batch/s, Loss: 2.5095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, [LOSS] train: 2.5011, val: 2.5082\n",
      "[QUESTION] To whom did the  National Institute of Architects release its report on the design of the World Trade Center? [ANSWER] the American Red Cross \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train:  99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 2700/2738 [07:31<00:06,  5.98 batch/s, Loss: 2.2990]\n",
      "Epoch: 2, Eval:  91%|███████████████████████████████████████████████████████████████████████████████████████████████████████▎          | 300/331 [00:15<00:01, 18.80 batch/s, Loss: 2.5110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, [LOSS] train: 2.2989, val: 2.5085\n",
      "[QUESTION] To whom did the  \"No Surprises\" campaign take place in the US? [ANSWER] the AFL \n"
     ]
    }
   ],
   "source": [
    "start_training(start_epoch=START_EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bfba8f-2ef1-4ba6-bb34-1aa95ea8db88",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2691c23a-303e-48b1-9a44-9b7311121f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QUESTION] How much did Beyonce initially contribute to the foundation? [ANSWER] $1 million \n"
     ]
    }
   ],
   "source": [
    "generate_text(\n",
    "    model=model, \n",
    "    device=device,\n",
    "    n_tokens=75, \n",
    "    temp=0.55, \n",
    "    context='[QUESTION] How much did Beyonce initially contribute to the foundation? [ANSWER]',\n",
    "    topk=50,\n",
    "    remove_newlines=False,\n",
    "    skip_after_end_token=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "36b0c82a-8ea6-4a60-8994-f3a633dfe0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QUESTION] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [ANSWER] Pope Innocent III \n"
     ]
    }
   ],
   "source": [
    "generate_text(\n",
    "    model=model, \n",
    "    device=device,\n",
    "    n_tokens=75, \n",
    "    temp=0.55, \n",
    "    context='[QUESTION] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [ANSWER]',\n",
    "    topk=50,\n",
    "    remove_newlines=False,\n",
    "    skip_after_end_token=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1764176-55be-4bd9-8772-ef9f3fe8e9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QUESTION] What color are the grass? [ANSWER] green \n"
     ]
    }
   ],
   "source": [
    "generate_text(\n",
    "    model=model, \n",
    "    device=device,\n",
    "    n_tokens=75, \n",
    "    temp=0.55, \n",
    "    context='[QUESTION] What color are the grass? [ANSWER]',\n",
    "    topk=50,\n",
    "    remove_newlines=False,\n",
    "    skip_after_end_token=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a841c8f6-c986-42ba-9f3d-55a01e28aa6d",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70680b96-64db-4778-9b3f-209a9259112c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: QA_GPT2_SQuAD_FineTune is saved successfully\n"
     ]
    }
   ],
   "source": [
    "save_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer, \n",
    "    root_folder='./saved_models/', \n",
    "    file_name=hyperparameters['model_base_name'], \n",
    "    hyperparameter_dict=hyperparameters, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a841ec84-3e12-4916-93ae-8e6c9feb3ce4",
   "metadata": {},
   "source": [
    "# The End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865813f0-22ad-4702-911d-ebb0bf57ac4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b252f5e8-6e26-4e4c-a911-7e9f813699aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
